{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Linear equations and matrix inverses (BV Chapter 11)\n",
    "subtitle: Biostat 216\n",
    "author: Dr. Hua Zhou @ UCLA\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    embed-resources: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left\n",
    "    code-fold: false\n",
    "    link-external-icon: true\n",
    "    link-external-newwindow: true\n",
    "comments:\n",
    "  hypothesis: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Documents/github.com/ucla-biostat-216/2024fall/slides/07-matinv/Project.toml`\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/github.com/ucla-biostat-216/2024fall/slides/07-matinv`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized inverse and Moore-Penrose inverse\n",
    "\n",
    "- Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. A matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times m}$ is called the **Moore-Penrose inverse** or **MP inverse** of $\\mathbf{A}$ if it satisifes following four conditions:   \n",
    "    (1) $\\mathbf{A} \\mathbf{G} \\mathbf{A} = \\mathbf{A}$. $\\quad \\quad$ (_generalized inverse_, $g_1$ inverse, or _inner pseudo-inverse_).   \n",
    "    (2) $\\mathbf{G} \\mathbf{A} \\mathbf{G} = \\mathbf{G}$. $\\quad \\quad$ (_outer pseudo-inverse_).    \n",
    "    (3) $(\\mathbf{A} \\mathbf{G})' = \\mathbf{A} \\mathbf{G}$.  \n",
    "    (4) $(\\mathbf{G} \\mathbf{A})' = \\mathbf{G} \\mathbf{A}$.  \n",
    "    We shall denote the **Moore-Penrose inverse** of $\\mathbf{A}$ by $\\mathbf{A}^+$.\n",
    "\n",
    "- Any matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times m}$ that satisfies (1) is called a **generalized inverse**, or **$g_1$ inverse**, or **inner pseudo-inverse**. Denoted by $\\mathbf{A}^-$ or $\\mathbf{A}^g$.  \n",
    "\n",
    "    Generalized inverse is not unique in general.\n",
    "\n",
    "- Definition (1) for generalized inverses is motivated by the following result.\n",
    "\n",
    "    If $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ has solution(s), then $\\mathbf{A}^- \\mathbf{b}$ is a solution for any generalized inverse $\\mathbf{A}^-$. \n",
    "\n",
    "    Proof: $\\mathbf{A} (\\mathbf{A}^- \\mathbf{b}) = \\mathbf{A} (\\mathbf{A}^- \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} \\mathbf{A}^- \\mathbf{A}) \\mathbf{x} = \\mathbf{A} \\mathbf{x} = \\mathbf{b}$. \n",
    "\n",
    "- Any matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times m}$ that satisfies (1)+(2) is called a **reflective generalized inverse**, or **$g_2$ inverse**, or **outer pseudo-inverse**. Denoted by $\\mathbf{A}^*$.\n",
    "\n",
    "- The **Moore-Penrose inverse** of any matrix $\\mathbf{A}$ exists and is unique. \n",
    "\n",
    "    Proof of uniqueness (optional): Let $\\mathbf{G}_1, \\mathbf{G}_2 \\in \\mathbb{R}^{n \\times m}$ be two matrices satisfying properties (1)-(4). Then  \n",
    "$$\n",
    "    \\mathbf{A} \\mathbf{G}_1 = (\\mathbf{A} \\mathbf{G}_1)' = \\mathbf{G}_1' \\mathbf{A}' = \\mathbf{G}_1' (\\mathbf{A} \\mathbf{G}_2 \\mathbf{A})' = \\mathbf{G}_1' \\mathbf{A}' \\mathbf{G}_2' \\mathbf{A}' = (\\mathbf{A} \\mathbf{G}_1)' (\\mathbf{A} \\mathbf{G}_2)' = \\mathbf{A} \\mathbf{G}_1 \\mathbf{A} \\mathbf{G}_2 = \\mathbf{A} \\mathbf{G}_2.\n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "    \\mathbf{G}_1 \\mathbf{A} = (\\mathbf{G}_1 \\mathbf{A})' = \\mathbf{A}' \\mathbf{G}_1' = (\\mathbf{A} \\mathbf{G}_2 \\mathbf{A})' \\mathbf{G}_1' = \\mathbf{A}' \\mathbf{G}_2' \\mathbf{A}' \\mathbf{G}_1' = (\\mathbf{G}_2 \\mathbf{A})' (\\mathbf{G}_1 \\mathbf{A})' = \\mathbf{G}_2 \\mathbf{A} \\mathbf{G}_1 \\mathbf{A} = \\mathbf{G}_2 \\mathbf{A}.\n",
    "$$\n",
    "Hence,\n",
    "$$\n",
    "    \\mathbf{G}_1 = \\mathbf{G}_1 \\mathbf{A} \\mathbf{G}_1 = \\mathbf{G}_1 \\mathbf{A} \\mathbf{G}_2 = \\mathbf{G}_2 \\mathbf{A} \\mathbf{G}_2 = \\mathbf{G}_2.\n",
    "$$\n",
    "    \n",
    "    Proof of existence: TODO later. We construct a matrix that satisfies (1)-(4) using the singular value decomposition (SVD) of $\\mathbf{A}$.\n",
    "    \n",
    "- Following are true: \n",
    "    1. $(\\mathbf{A}^-)'$ is a generalized inverse of $\\mathbf{A}'$. \n",
    "    2. $(\\mathbf{A}^+)' = (\\mathbf{A}')^+$.  \n",
    "    3. For any nonzero $k$, $(1/k) \\mathbf{A}^-$ is a generalized inverse of $k \\mathbf{A}$.  \n",
    "    4. $(k \\mathbf{A})^+ = (1/k) \\mathbf{A}^+$.\n",
    "\n",
    "    Proof: Check properties (1)-(4). HW5.\n",
    "    \n",
    "- **Multiplication by generalized inverse does not change rank.**  \n",
    "    1. $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{A} \\mathbf{A}^-)$ and $\\mathcal{C}(\\mathbf{A}') = \\mathcal{C}((\\mathbf{A}^- \\mathbf{A})')$.  \n",
    "    2. $\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A} \\mathbf{A}^-) = \\text{rank}(\\mathbf{A}^- \\mathbf{A})$. \n",
    "    \n",
    "    Proof of 1: We already know $\\mathcal{C}(\\mathbf{A}) \\supseteq \\mathcal{C}(\\mathbf{A} \\mathbf{A}^-)$. Now since $\\mathbf{A} = \\mathbf{A} \\mathbf{A}^- \\mathbf{A}$, we also have $\\mathcal{C}(\\mathbf{A}) \\subseteq \\mathcal{C}(\\mathbf{A} \\mathbf{A}^-)$.   \n",
    "    Proof of 2: Since $\\mathbf{A} = \\mathbf{A} \\mathbf{A}^- \\mathbf{A}$, $\\text{rank}(\\mathbf{A}) \\le \\text{rank}(\\mathbf{A} \\mathbf{A}^-) \\le \\text{rank}(\\mathbf{A})$.  \n",
    "    \n",
    "- **Generalized inverse has equal or larger rank.**  $\\text{rank}(\\mathbf{A}^-) \\ge \\text{rank}(\\mathbf{A})$.  \n",
    "\n",
    "    Proof: $\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A}\\mathbf{A}^- \\mathbf{A}) \\le \\text{rank}(\\mathbf{A}\\mathbf{A}^-) \\le \\text{rank}(\\mathbf{A}^-)$.\n",
    "\n",
    "- **Moore-Penrose inverse has equal rank.**  $\\text{rank}(\\mathbf{A}^+) = \\text{rank}(\\mathbf{A})$.  \n",
    "\n",
    "    Proof: HW5.\n",
    "    \n",
    "- Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "    1. If $\\mathbf{A}$ has full column rank, then the MP inverse is $\\mathbf{A}^+ = (\\mathbf{A}'\\mathbf{A})^{-1} \\mathbf{A}'$. Given QR factorization $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$, $\\mathbf{A}^+ = \\mathbf{R}^{-1} \\mathbf{Q}'$.    \n",
    "    \n",
    "    2. If $\\mathbf{A}$ has full row rank, then the MP inverse is $\\mathbf{A}^+ = \\mathbf{A}'(\\mathbf{A} \\mathbf{A}')^{-1}$.  Given QR factorization $\\mathbf{A}' = \\mathbf{Q} \\mathbf{R}$, $\\mathbf{A}^+ = \\mathbf{Q} \\mathbf{R}'^{-1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Int64}:\n",
       " -3  -4\n",
       "  4   6\n",
       "  1   1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 3x2 matrix (full column rank)\n",
    "A = [-3 -4; 4 6; 1 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Matrix{Float64}:\n",
       " -1.22222   -1.11111    1.77778\n",
       "  0.777778   0.888889  -1.22222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute MP inverse using SVD\n",
    "pinv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Matrix{Float64}:\n",
       " -1.22222   -1.11111    1.77778\n",
       "  0.777778   0.888889  -1.22222"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute MP inverse using pseudo-inverse\n",
    "(A'A) \\ A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\n",
       "Q factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\n",
       "R factor:\n",
       "2×2 Matrix{Float64}:\n",
       " 5.09902   7.2563\n",
       " 0.0      -0.588348"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute MP inverse by QR\n",
    "qra = qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Matrix{Float64}:\n",
       " -1.22222   -1.11111    1.77778\n",
       "  0.777778   0.888889  -1.22222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qra.R \\ qra.Q[:, 1:2]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear equations\n",
    "\n",
    "- Solving linear system\n",
    "\\begin{eqnarray*}\n",
    "a_{11} x_1 + \\cdots + a_{1n} x_n &=& b_1 \\\\\n",
    "a_{21} x_1 + \\cdots + a_{2n} x_n &=& b_2 \\\\\n",
    "&\\vdots& \\\\\n",
    "a_{m1} x_1 + \\cdots + a_{mn} x_n &=& b_m\n",
    "\\end{eqnarray*}\n",
    "or\n",
    "$$\n",
    "    \\mathbf{A} \\mathbf{x} = \\mathbf{b},\n",
    "$$\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ (coefficient matrix), $\\mathbf{x} \\in \\mathbb{R}^n$ (solution vector), and $\\mathbf{b} \\in \\mathbb{R}^m$ (right hand side) is a central theme in linear algebra. \n",
    "\n",
    "\n",
    "### When is there a solution to $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$?\n",
    "\n",
    "- Theorem: The following statements are equivalent.\n",
    "\n",
    "    1. The linear system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ has a solution (**consistent**).\n",
    "\n",
    "    2. $\\mathbf{b} \\in {\\cal C}(\\mathbf{A})$.\n",
    " \n",
    "    3. $\\text{rank}((\\mathbf{A} \\,\\, \\mathbf{b})) = \\text{rank}(\\mathbf{A})$.\n",
    "    \n",
    "    4. $\\mathbf{A} \\mathbf{A}^- \\mathbf{b} = \\mathbf{b}$.\n",
    "\n",
    "    Proof: Equivalence between 1, 2 and 3 is trivial. 4 implies 1: apparently $\\tilde{\\mathbf{x}} = \\mathbf{A}^- \\mathbf{b}$ is a solution to $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$. 1 implies 4: if $\\tilde{\\mathbf{x}}$ is a solution, then $\\mathbf{b} = \\mathbf{A} \\tilde{\\mathbf{x}} = \\mathbf{A} \\mathbf{A}^- \\mathbf{A} \\tilde{\\mathbf{x}} = \\mathbf{A} \\mathbf{A}^- \\mathbf{b}$.\n",
    "\n",
    "    The last equivalence gives some intuition why $\\mathbf{A}^-$ is called an inverse.\n",
    "\n",
    "### How to characterize all solutions to $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$?\n",
    "\n",
    "- Let's first study the homogenous case $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$, which is always consistent (why?).\n",
    "\n",
    "    Theorem: $\\tilde{\\mathbf{x}}$ is a solution to $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$ if and only if $\\tilde{\\mathbf{x}} = (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q}$ for some $\\mathbf{q} \\in \\mathbb{R}^n$.\n",
    "\n",
    "    Proof: _If_: Apparently $(\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q}$ is a solution regardless value of $\\mathbf{q}$ since $\\mathbf{A} (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) = \\mathbf{A} - \\mathbf{A} = \\mathbf{0}$. _Only if_: If $\\tilde{\\mathbf{x}}$ is a solution, then $\\tilde{\\mathbf{x}} = (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q}$ by taking $\\mathbf{q} = \\tilde{\\mathbf{x}}$.\n",
    "\n",
    "- Rephrasing above result, we have $\\mathcal{N}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A})$. \n",
    "\n",
    "- Remarks (optional):\n",
    "\n",
    "    1. The matrix $\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}$ is idempotent and is an oblique projection onto $\\mathcal{N}(\\mathbf{A})$.\n",
    " \n",
    "    2. The matrix $\\mathbf{I}_n - \\mathbf{A}^+ \\mathbf{A}$ is symmetric and idempotent and is an orthogonal projection onto $\\mathcal{N}(\\mathbf{A})$.\n",
    " \n",
    "    3. The matrix $\\mathbf{A} \\mathbf{A}^-$ is idempotent and is an oblique projection onto $\\mathcal{C}(\\mathbf{A})$.\n",
    " \n",
    "    4. The matrix $\\mathbf{A} \\mathbf{A}^+$ is symmetric and idempotent and is an orthogonal projection onto $\\mathcal{C}(\\mathbf{A})$.\n",
    "\n",
    "- Now for the inhomogenous case $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "    Theorem: If $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is consistent, then $\\tilde{\\mathbf{x}}$ is a solution if and only if\n",
    "    $$\n",
    "      \\tilde{\\mathbf{x}} = \\mathbf{A}^- \\mathbf{b} + (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q} \n",
    "    $$\n",
    "    for some $\\mathbf{q} \\in \\mathbb{R}^n$.\n",
    "\n",
    "    Interpretation: **a specific solution** + **a vector in $\\mathcal{N}(\\mathbf{A})$**.\n",
    "\n",
    "    Proof:\n",
    "    \\begin{eqnarray*}\n",
    "    & & \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{A} \\mathbf{x} = \\mathbf{A} \\mathbf{A}^- \\mathbf{b} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{A} (\\mathbf{x} - \\mathbf{A}^- \\mathbf{b}) = \\mathbf{0} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} - \\mathbf{A}^- \\mathbf{b} = (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q} \\text{ for some } \\mathbf{q} \\in \\mathbb{R}^n \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} = \\mathbf{A}^- \\mathbf{b} + (\\mathbf{I}_n - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q} \\text{ for some } \\mathbf{q} \\in \\mathbb{R}^n.\n",
    "    \\end{eqnarray*}\n",
    "\n",
    "- Corollary: $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is consistent for _all_ $\\mathbf{b}$ if and only if $\\mathbf{A}$ has full row rank.\n",
    "\n",
    "- Corollary: If a system is consistent, its solution is unique if and only if $\\mathbf{A}$ has full column rank.\n",
    "\n",
    "## Invertible matrix\n",
    "\n",
    "Assume that a square $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ has full row and column rank.\n",
    "\n",
    "- If a square $\\mathbf{A}$ has full row and column rank, then $\\mathbf{A}$ is **invertible** (or **nonsingular**).\n",
    "\n",
    "- Consider the left inverse $\\mathbf{X}$ that satisifies $\\mathbf{X} \\mathbf{A} = \\mathbf{I}$ and the right inverse $\\mathbf{Y}$ that satisfies $\\mathbf{A} \\mathbf{Y} = \\mathbf{I}$. Both left and right inverses exist (why?) and they are equal:\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{X} \\mathbf{I} = \\mathbf{X} \\mathbf{A} \\mathbf{Y} = \\mathbf{I} \\mathbf{Y} = \\mathbf{Y}.\n",
    "$$\n",
    "Uniqueness of the left and right inverse follows from uniqueness of the basis expansion. \n",
    "\n",
    "- We say that $\\mathbf{A}$ is **invertible** (or **nonsingular**), and denote the (left and right) **inverse** by $\\mathbf{A}^{-1}$.\n",
    "\n",
    "- The generalized inverse $\\mathbf{A}^-$ is unique, which is the same as $\\mathbf{A}^{-1}$.\n",
    "\n",
    "    Proof: Let $\\mathbf{G}_1$ and $\\mathbf{G}_2$ be two generalized inverses of $\\mathbf{A}$. Then $\\mathbf{A} \\mathbf{G}_1 \\mathbf{A} = \\mathbf{A}$ and $\\mathbf{A} \\mathbf{G}_2 \\mathbf{A} = \\mathbf{A}$. Multiplying both equations by $\\mathbf{A}^{-1}$ on the left and right, we obtain $\\mathbf{G}_1 = \\mathbf{A}^{-1} = \\mathbf{G}_2$.\n",
    "\n",
    "- For any $\\mathbf{b}$, the unique solution is $\\mathbf{A}^{-1} \\mathbf{b}$.\n",
    "\n",
    "### Examples of matrix inverse\n",
    "\n",
    "- Identity matrix $\\mathbf{I}_n$ is invertible, with inverse $\\mathbf{I}_n$. \n",
    "\n",
    "- A diagonal matrix is invertible if all diagonal entries are nonzero.\n",
    "$$\n",
    "\\text{diag}(a_{11}, \\ldots, a_{nn})^{-1} = \\text{diag}(a_{11}^{-1}, \\ldots, a_{nn}^{-1}).\n",
    "$$\n",
    "\n",
    "- A $2 \\times 2$ matrix is invertible if and only if $a_{11} a_{22} \\ne a_{12} a_{21}$\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}^{-1} = \\frac{1}{a_{11} a_{22} - a_{12} a_{21}} \\begin{pmatrix}\n",
    "a_{22} & -a_{12} \\\\\n",
    "-a_{21} & a_{11}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "- Inverse of orthogonal matrix. If $\\mathbf{A}$ is square and has orthonormal columns, then\n",
    "$$\n",
    "\\mathbf{A}^{-1} = \\mathbf{A}'.\n",
    "$$\n",
    "\n",
    "- Inverse of matrix transpose. If $\\mathbf{A}$ is invertible, then\n",
    "$$\n",
    "(\\mathbf{A}')^{-1} = (\\mathbf{A}^{-1})'.\n",
    "$$\n",
    "\n",
    "- Inverse of matrix product. If $\\mathbf{A}$ and $\\mathbf{B}$ are invertible, then\n",
    "$$\n",
    "(\\mathbf{A} \\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}.\n",
    "$$  \n",
    "\n",
    "## Solving linear equations - triangular systems\n",
    "\n",
    "- To solve $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **lower triangular**\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    a_{11} & 0 & \\cdots & 0 \\\\\n",
    "    a_{21} & a_{22} & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "    **Forward substitution** algorithm: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    x_1 &=& b_1 / a_{11} \\\\\n",
    "    x_2 &=& (b_2 - a_{21} x_1) / a_{22} \\\\\n",
    "    x_3 &=& (b_3 - a_{31} x_1 - a_{32} x_2) / a_{33} \\\\\n",
    "    &\\vdots& \\\\\n",
    "    x_n &=& (b_n - a_{n1} x_1 - a_{n2} x_2 - \\cdots - a_{n,n-1} x_{n-1}) / a_{nn}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$1 + 3 + 5 + \\cdots + (2n-1) = n^2$ flops. \n",
    "\n",
    "- To solve $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is upper triangular  \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    a_{11} & \\cdots & a_{1,n-1} & a_{1n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0 & \\cdots & a_{n-1,n-1} & a_{n-1,n} \\\\\n",
    "    0 & 0 & 0 & a_{nn}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "b_1 \\\\ \\vdots \\\\ b_{n-1} \\\\ b_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "    **Back substitution** algorithm: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    x_n &=& b_n / a_{nn} \\\\\n",
    "    x_{n-1} &=& (b_{n-1} - a_{n-1,n} x_n) / a_{n-1,n-1} \\\\\n",
    "    x_{n-2} &=& (b_{n-2} - a_{n-2,n-1} x_{n-1} - a_{n-2,n} x_n) / a_{n-2,n-2} \\\\\n",
    "    &\\vdots& \\\\\n",
    "    x_1 &=& (b_1 - a_{12} x_2 - a_{13} x_3 - \\cdots - a_{1,n} x_{n}) / a_{11}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$n^2$ flops.\n",
    "\n",
    "## Solving linear equations by QR\n",
    "\n",
    "- For an invertible matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^n$, we can solve the linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ by following steps.  \n",
    "    - Step 1: Compute QR factorization, e.g., by [Gram–Schmidt](https://ucla-biostat-216.github.io/2023fall/slides/02-vector/02-vector.html#how-to-orthonormalize-a-set-of-vectors-gram-schmidt-algorithm) or [Household algorithm](https://ucla-biostat-216.github.io/2023fall/hw/hw4/hw4.html#q3-household-reflections) (HW5): $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$. $2n^3$ flops.  \n",
    "    - Step 2: Compute $\\mathbf{Q}' \\mathbf{b}$. $2n^2$ flops.  \n",
    "    - Step 3: Solve the triangular equation $\\mathbf{R} \\mathbf{x} = \\mathbf{Q}' \\mathbf{b}$ by back substitution. $n^2$ flops.   \n",
    "    \n",
    "    The total number of flops is $2n^3 + 3n^2 \\approx 2n^3$.\n",
    "    \n",
    "- Factor-solve method with multiple right-hand sides. For multiple right hand sides $\\mathbf{b}$, we only need to do Step 1 once.\n",
    "\n",
    "- Compute the inverse, $\\mathbf{B}$, of an invertible matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$.  \n",
    "    - Step 1. QR factorization $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$.  \n",
    "    - Step 2. For $i=1,\\ldots,n$, solve the triangular equation $\\mathbf{R} \\mathbf{b}_i = \\mathbf{s}_i$, where $\\mathbf{s}_i$ is the $i$-th row of $\\mathbf{Q}$. \n",
    "    \n",
    "    Total computational cost is $2n^3 + n^3 = 3n^3$ flops.  \n",
    "    \n",
    "- Question. Two ways to solve a linear system. `qr(A) \\ b` vs `inv(A) * b`. Which way is better?\n",
    "\n",
    "## Solving linear equations by Gaussian elimination and LU factorization\n",
    "\n",
    "In this section, we (1) review the Gaussian elimination (GE) for solving linear equations  $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, and then (2) show that GE leads to a useful decomposition \n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{L} \\mathbf{U}.\n",
    "$$\n",
    "\n",
    "Remark: In practice, LU decomposition is used much less frequently than Cholesky decomposition in statistics and machinear learning because $\\mathbf{A}$ is almost always symmetric and positive semidefinite in applications.\n",
    "\n",
    "### Gaussian elimination (GE)\n",
    "\n",
    "Let's review (from any undergraduate linear algebra textbook) how to solve a linear system\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & -1 \\\\ -3 & -1 & 2 \\\\ -2 & 1 & 2\n",
    "\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "8 \\\\ -11 \\\\ -3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Stage 1**: Let's eliminate variable $x_1$ from equations (2) and (3).\n",
    "\n",
    "Multiplying equation (1) by $\\ell_{21} = 3/2$ and adds to equation (2) leads to\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & -1 \\\\\n",
    "0 & 1/2 & 1/2 \\\\ -2 & 1 & 2\n",
    "\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "8 \\\\ 1 \\\\ -3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Multiplying equation (1) by $\\ell_{31} = 1$ and adds to equation (3) leads to\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & -1 \\\\\n",
    "0 & 1/2 & 1/2 \\\\\n",
    "0 & 2 & 1\n",
    "\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "8 \\\\ 1 \\\\ 5\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Stage 2**: Let's eliminate variable $x_2$ from equation (3).\n",
    "\n",
    "Multiplying equation (2) by $\\ell_{32} = -4$ and adds to equation (3) leads to\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & -1 \\\\\n",
    "0 & 1/2 & 1/2 \\\\\n",
    "0 & 0 & -1\n",
    "\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "8 \\\\ 1 \\\\ 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Stage 3**: Now we can collect results by **back-solve** or **back-substitution**.\n",
    "\n",
    "From the equation (3), $x_3 = -1$.  \n",
    "\n",
    "Substituting $x_3 = -1$ to equation (2) and solve for $x_2 = 3$.\n",
    "\n",
    "Substituting $x_2=3$ and $x_3 = 1$ to equation (1) and solve for $x_2 = 2$.\n",
    "\n",
    "This is essentially how computer solves linear equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  2.0   1.0  -1.0\n",
       " -3.0  -1.0   2.0\n",
       " -2.0   1.0   2.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "A = [2.0 1.0 -1.0; -3.0 -1.0 2.0; -2.0 1.0 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "   8.0\n",
       " -11.0\n",
       "  -3.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [8.0, -11.0, -3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  2.0000000000000004\n",
       "  2.9999999999999996\n",
       " -0.9999999999999991"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Julia way to solve linear equation\n",
    "# equivalent to `solve(A, b)` in R\n",
    "A \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU decomposition\n",
    "\n",
    "Let's collect those multipliers $-\\ell_{ij}$ into a unit lower triangular matrix $\\mathbf{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  1.0  0.0  0.0\n",
       " -1.5  1.0  0.0\n",
       " -1.0  4.0  1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [1.0 0.0 0.0; -1.5 1.0 0.0; -1.0 4.0 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the resultant upper triangular matrix after GE into $\\mathbf{U}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 2.0  1.0  -1.0\n",
       " 0.0  0.5   0.5\n",
       " 0.0  0.0  -1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = [2.0 1.0 -1.0; 0.0 0.5 0.5; 0.0 0.0 -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we find that $\\mathbf{A} = \\mathbf{L} \\mathbf{U}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A ≈ L * U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theorem: For any nonsingular $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, there exists a unique unit lower triangular matrix $\\mathbf{L}$ and upper triangular matrix $\\mathbf{U}$ such that\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{L} \\mathbf{U}.\n",
    "$$\n",
    "\n",
    "- Given LU decomposition $\\mathbf{A} = \\mathbf{L} \\mathbf{U}$, for a new right hand side $\\mathbf{b}$, the solution to $\\mathbf{A} \\mathbf{x} = \\mathbf{L} \\mathbf{U} \\mathbf{x} = \\mathbf{b}$ is readily given by two triangular solves:\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{L} \\mathbf{y} &=& \\mathbf{b} \\\\\n",
    "    \\mathbf{U} \\mathbf{x} &=& \\mathbf{y}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Indeed, computer performs GE/LU on a row-permuted version of $\\mathbf{A}$ (pivoting) for numerical stability. That is \n",
    "$$\n",
    "\\mathbf{P} \\mathbf{A} = \\mathbf{L} \\mathbf{U},\n",
    "$$\n",
    "where $\\mathbf{P}$ is a permutation matrix. All multipliers $\\ell_{ij}$ in $\\mathbf{L}$ has magnitude $\\le 1$.\n",
    "\n",
    "- Total computational cost of LU decomposition is $(2/3)n^3$ flops.\n",
    "\n",
    "- Remark: Total computational cost of the Cholesky decomposition is $(1/3)n^3$ flops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  2.0   1.0  -1.0\n",
       " -3.0  -1.0   2.0\n",
       " -2.0   1.0   2.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU{Float64, Matrix{Float64}, Vector{Int64}}\n",
       "L factor:\n",
       "3×3 Matrix{Float64}:\n",
       "  1.0       0.0  0.0\n",
       "  0.666667  1.0  0.0\n",
       " -0.666667  0.2  1.0\n",
       "U factor:\n",
       "3×3 Matrix{Float64}:\n",
       " -3.0  -1.0      2.0\n",
       "  0.0   1.66667  0.666667\n",
       "  0.0   0.0      0.2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alu = lu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alu.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[Alu.p, :] ≈ Alu.L * Alu.U"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "589px",
    "left": "0px",
    "right": "820.5px",
    "top": "33.2px",
    "width": "175.2px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
