{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Least squares (BV Chapters 12-13)\n",
    "subtitle: Biostat 216\n",
    "author: Dr. Hua Zhou @ UCLA\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    embed-resources: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left\n",
    "    code-fold: false\n",
    "    link-external-icon: true\n",
    "    link-external-newwindow: true\n",
    "comments:\n",
    "  hypothesis: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of least squares\n",
    "\n",
    "- In the previous lecture, we study when does a linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ have solution and how to characterize all the solutions.  \n",
    "  \n",
    "- What if $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is **inconsistent**, i.e., there is no solution?  \n",
    "\n",
    "    A sensible question to ask is can we find an $\\mathbf{x}$ such that $\\mathbf{A} \\mathbf{x}$ approximates $\\mathbf{b}$ best? The **method of least squares** (due to Gauss) seeks to minimize\n",
    "\\begin{eqnarray*}\n",
    "Q(\\mathbf{x}) &=& \\|\\mathbf{b} - \\mathbf{A} \\mathbf{x}\\|^2 \\\\\n",
    "&=& (\\mathbf{b} - \\mathbf{A} \\mathbf{x})' (\\mathbf{b} - \\mathbf{A} \\mathbf{x}) \\\\\n",
    "&=& \\mathbf{b}' \\mathbf{b} - 2 \\mathbf{b}' \\mathbf{A} \\mathbf{x} + \\mathbf{x}' \\mathbf{A}' \\mathbf{A} \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- **Normal equation**. To find the minimum, we take derivative and set the gradient to zero\n",
    "$$\n",
    "\\nabla Q(\\mathbf{x}) = -2 \\mathbf{A}' \\mathbf{b} + 2 \\mathbf{A}' \\mathbf{A} \\mathbf{x} = \\mathbf{0}.\n",
    "$$\n",
    "This leads to the normal equation\n",
    "$$\n",
    "\\mathbf{A}' \\mathbf{A} \\mathbf{x} = \\mathbf{A}' \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "## Least squares solution\n",
    "\n",
    "- Is there a solution to the normal equation? This is answered in the last lecture and HW5.\n",
    "\n",
    "- Is the solution to the normal equation the minimizer to the least sqaures criterion $Q(\\mathbf{x})$?\n",
    "\n",
    "    Answer: Any solution $\\mathbf{x}$ to the normal equation minimizes the least squares criterion.\n",
    "\n",
    "    Optimization argument: Any stationarity point (points with zero gradient vector) of a convex function is a global minimum. Now the least squares criterion is convex because the Hessian  $\\nabla^2 Q(\\mathbf{x}) = \\mathbf{A}' \\mathbf{A}$ is positive semidefinite. Therefore any solution to the normal equation is a stationarity point and thus a global minimum.\n",
    "\n",
    "    Direct argument: Let $\\hat{\\mathbf{x}}$ be a solution to the normal equation. That is\n",
    "\\begin{eqnarray*}\n",
    "& & \\mathbf{A}' \\mathbf{A} \\hat{\\mathbf{x}} = \\mathbf{A}' \\mathbf{b} \\\\\n",
    "&\\Rightarrow& \\mathbf{A}' (\\mathbf{b} - \\mathbf{A} \\hat{\\mathbf{x}}) = \\mathbf{0} \\\\\n",
    "&\\Rightarrow& \\mathbf{b} - \\mathbf{A} \\hat{\\mathbf{x}} \\text{ is orthogonal to all columns of } \\mathbf{A} \\\\\n",
    "&\\Rightarrow& \\mathbf{b} - \\mathbf{A} \\hat{\\mathbf{x}} \\in \\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}').\n",
    "\\end{eqnarray*}\n",
    "Therefore $\\mathbf{b} = \\mathbf{A} \\hat{\\mathbf{x}} + (\\mathbf{b} - \\mathbf{A} \\hat{\\mathbf{x}})$ where $\\mathbf{A} \\hat{\\mathbf{x}} \\in \\mathcal{C}(\\mathbf{A})$ and $\\mathbf{b} - \\mathbf{A} \\hat{\\mathbf{x}} \\in \\mathcal{C}(\\mathbf{A})^\\perp$. In other words, $\\mathbf{A} \\hat{\\mathbf{x}}$ is the orthogonal projection of $\\mathbf{b}$ into $\\mathcal{C}(\\mathbf{A})$. Then by the cloest point theorem, we know $Q(\\mathbf{x})$ is minimized by $\\hat{\\mathbf{x}}$.\n",
    "\n",
    "- The direct argument also reveals that the _fitted values_ $\\hat{\\mathbf{b}} = \\mathbf{A} \\hat{\\mathbf{x}}$ is invariant to the choice of the solution to the normal equation.\n",
    "\n",
    "- Now we know the normal equation is always consistent and we want to find all solution(s). In general the least squares solution can be represented as\n",
    "\\begin{eqnarray*}\n",
    "& & (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}' \\mathbf{b} + [\\mathbf{I} - (\\mathbf{A}' \\mathbf{A})^- (\\mathbf{A}' \\mathbf{A})] \\mathbf{q} \\\\\n",
    "&=& (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}' \\mathbf{b} + (\\mathbf{I} - \\mathbf{A}^- \\mathbf{A}) \\mathbf{q},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{q}$ is arbitrary. One specific solution is \n",
    "\\begin{eqnarray*}\n",
    "\t\\hat{\\mathbf{x}} = (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}' \\mathbf{b}\n",
    "\\end{eqnarray*}\n",
    "with corresponding fitted values \n",
    "\\begin{eqnarray*}\n",
    "\t\\hat{\\mathbf{b}} = \\mathbf{A} (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}' \\mathbf{b}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- When is the least squares solution unique?  \n",
    "    The least squares solution is unique if and only if $\\mathbf{A}$ has full column rank. The solution is given by $\\hat{\\mathbf{x}} = (\\mathbf{A}' \\mathbf{A})^{-1} \\mathbf{A}' \\mathbf{b}$.  \n",
    "\n",
    "    Proof: The solution to normal equation is unique if and only if $\\mathbf{A}' \\mathbf{A}$ has full (column) rank. Therefore $\\mathbf{A}$ has full column rank as well.\n",
    "\n",
    "## Geometry of the least squares solution\n",
    "\n",
    "<img src=\"./ls_projection.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- $(\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}'$ is a generalized inverse of $\\mathbf{A}$.\n",
    "\n",
    "    Proof: HW5.\n",
    "\n",
    "- $\\mathbf{P}_{\\mathbf{A}} = \\mathbf{A} (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}'$ is the orthogonal projector onto $\\mathcal{C}(\\mathbf{A})$.\n",
    "\n",
    "    Proof: HW5.\n",
    "\n",
    "    Since orthogonal projection is unique, $\\mathbf{A} (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}'$ is invariant to the choice of the generalized inverse $(\\mathbf{A}' \\mathbf{A})^-$ and thus can be denoted by $\\mathbf{P}_{\\mathbf{A}}$.\n",
    "\n",
    "- Whichever least squares solution $\\hat{\\mathbf{x}}$ we use, the fitted value $\\hat{\\mathbf{b}} = \\mathbf{A} \\hat{\\mathbf{x}} = \\mathbf{A} (\\mathbf{A}' \\mathbf{A})^- \\mathbf{A}' \\mathbf{b} = \\mathbf{P}_{\\mathbf{A}} \\mathbf{b}$ is the same.\n",
    "\n",
    "- Geometry: The fitted value from the least squares solution $\\hat{\\mathbf{b}} = \\mathbf{P}_{\\mathbf{A}} \\mathbf{b}$ is the orthogonal projection of the response vector $\\mathbf{b}$ onto the column space $\\mathcal{C}(\\mathbf{A})$.\n",
    "\n",
    "- Decomposition of $\\mathbf{b} = \\mathbf{P}_{\\mathbf{A}} \\mathbf{b} + (\\mathbf{I} - \\mathbf{P}_{\\mathbf{A}}) \\mathbf{b} = \\hat{\\mathbf{b}} + \\hat{\\mathbf{e}}$, where $\\hat{\\mathbf{b}} \\perp \\hat{\\mathbf{e}}$.\n",
    "\n",
    "## Solve least squares problems by QR\n",
    "\n",
    "Assume $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ has full column rank and $\\mathbf{b} \\in \\mathbb{R}^m$.\n",
    "\n",
    "- Step 1: Compute the (thin) QR factorization e.g., by [Gramâ€“Schmidt](https://ucla-biostat-216.github.io/2024fall/slides/02-vector/02-vector.html#how-to-orthonormalize-a-set-of-vectors-gram-schmidt-algorithm) or [Householder algorithm](https://ucla-biostat-216.github.io/2024fall/hw/hw5/hw5.html#q4-household-reflections) (HW5): $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$.\n",
    "\n",
    "- Step 2: Compute $\\mathbf{Q}' \\mathbf{b}$.\n",
    "\n",
    "- Step 3: Back substitution: Solve triangular system $\\mathbf{R} \\hat{\\mathbf{x}} = \\mathbf{Q}' \\mathbf{b}$.\n",
    "\n",
    "- Remark: The only difference of this algorithm from the [QR approach for solving linear equation](https://ucla-biostat-216.github.io/2024fall/slides/07-matinv/07-matinv.html#solving-linear-equations-by-qr) is that $\\mathbf{A}$ and $\\mathbf{Q}$ can be tall.\n",
    "\n",
    "- Total number of flops, using Gram-Schmidt, is $2mn^2 + 2mn + n^2 \\approx 2mn^2$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "589px",
    "left": "0px",
    "right": "820.5px",
    "top": "33.2px",
    "width": "175.2px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
